{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Processuales/FastOB_API/blob/main/FastOoba_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ðŸŽµ Run Silent Audio Player { display-mode: \"form\" }\n",
        "\n",
        "#@markdown Press play on the audio player that appears below. This will keep the Colab tab alive and prevent Google from disconnecting you for inactivity.\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "haXf9xM6jefN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 61
        },
        "outputId": "67aa687d-afcd-49f5-de2e-e941b1c57f69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Connect to Google Drive || https://drive.google.com/drive/u/0/my-drive { display-mode: \"form\" }\n",
        "#@markdown <| Make sure to have atleast 8GB of free space |>\n",
        "from google.colab import drive\n",
        "from IPython.display import clear_output\n",
        "import ipywidgets as widgets\n",
        "import os\n",
        "\n",
        "shared_drive = \"\" #@param {type:\"string\"}\n",
        "#@markdown - Leave empty if you're not using a shared drive\n",
        "\n",
        "def message(msg, style, wdth):\n",
        "  message = widgets.Button(description=msg, disabled=True, button_style=style, layout=widgets.Layout(min_width=wdth))\n",
        "  display(message)\n",
        "\n",
        "print(\"\u001b[0;33mConnecting...\")\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "if shared_drive!=\"\" and os.path.exists(\"/content/gdrive/Shareddrives\"):\n",
        "  main_path=\"/content/gdrive/Shareddrives/\"+shared_drive\n",
        "else:\n",
        "  main_path=\"/content/gdrive/MyDrive/\"\n",
        "\n",
        "clear_output()\n",
        "message('\\u2714 Connected','success', '50px')\n",
        "\n",
        "if not os.path.exists(\"/content/gdrive\"):\n",
        "  print('\u001b[1;31mGdrive not connected, using temporary colab storage ...')\n",
        "  main_path=\"/content/\"\n",
        "  shared_drive=\"\"\n",
        "\n",
        "if shared_drive != \"\" and not os.path.exists(\"/content/gdrive/Shareddrives\"):\n",
        "  print('\u001b[1;31mShared drive not detected, using default MyDrive')\n",
        "  main_path=\"/content/gdrive/MyDrive/\"\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "folder_name = \"Text-Gen\" #@param {type :\"string\"}\n",
        "text_gen_dir = f\"{main_path}{folder_name}\"\n",
        "#@markdown - Crates a folder by this name if it does not exist already.\n",
        "\n",
        "if not os.path.exists(text_gen_dir):\n",
        "    os.makedirs(text_gen_dir)\n",
        "    print(f\"\u001b[0;32mCreated Folder at : {text_gen_dir}/\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4Gyv2FBEK7i2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Initialize Oobabooga Text-Web-UI || https://github.com/oobabooga/text-generation-webui\n",
        "from IPython.display import clear_output\n",
        "Update_Every_Execution = True #@param {type:\"boolean\"}\n",
        "Download_Branch = False #@param {type:\"boolean\"}\n",
        "custom_web_path = \"\" #@param {type:\"string\"}\n",
        "#@markdown - You can leave this empty\n",
        "#@markdown ---\n",
        "use_cloudflare_public_api = True #@param {type :\"boolean\"}\n",
        "\n",
        "\n",
        "\n",
        "%cd {text_gen_dir}\n",
        "\n",
        "def message(msg, style, wdth):\n",
        "  message = widgets.Button(description=msg, disabled=True, button_style=style, layout=widgets.Layout(min_width=wdth))\n",
        "  display(message)\n",
        "\n",
        "def main_web():\n",
        "  if not os.path.exists(os.path.join(text_gen_dir, \"text-generation-webui\")): #\n",
        "    !git clone https://github.com/oobabooga/text-generation-webui\n",
        "    %cd {text_gen_dir}/text-generation-webui\n",
        "  else:\n",
        "    %cd {text_gen_dir}/text-generation-webui\n",
        "    if Update_Every_Execution:\n",
        "      !git pull origin\n",
        "\n",
        "def branch_web():\n",
        "  if not os.path.exists(os.path.join(text_gen_dir, \"text-generation-webui\")): #\n",
        "    !git clone -b v2.5 https://github.com/camenduru/text-generation-webui\n",
        "    %cd {text_gen_dir}/text-generation-webui\n",
        "  else:\n",
        "    %cd {text_gen_dir}/text-generation-webui\n",
        "    if Update_Every_Execution:\n",
        "      !git pull\n",
        "\n",
        "def custom_web(path_to_web):\n",
        "  if not os.path.exists(os.path.join(text_gen_dir, \"text-generation-webui\")): #\n",
        "    !git clone {path_to_web}\n",
        "    %cd {text_gen_dir}/text-generation-webui\n",
        "  else:\n",
        "    %cd {text_gen_dir}/text-generation-webui\n",
        "    if Update_Every_Execution:\n",
        "      !git pull\n",
        "\n",
        "if custom_web_path == \"\":\n",
        "  if not Download_Branch:\n",
        "    main_web()\n",
        "  else:\n",
        "    branch_web()\n",
        "else:\n",
        "  custom_web(custom_web_path)\n",
        "\n",
        "!pip install -q -r requirements.txt\n",
        "!pip install xformers\n",
        "if use_cloudflare_public_api:\n",
        "  !pip install flask-cloudflared\n",
        "\n",
        "!echo \"dark_theme: true\" > {text_gen_dir}/settings.yaml\n",
        "!echo \"chat_style: wpp\" >> {text_gen_dir}/settings.yaml\n",
        "!echo \"mode: 'instruct'\" >> {text_gen_dir}/settings.yaml\n",
        "\n",
        "\n",
        "script_file_path = os.path.join(text_gen_dir, \"text-generation-webui/extensions/api/script.py\")\n",
        "\n",
        "serveo_api_script = \"\"\"\n",
        "import extensions.api.blocking_api as blocking_api\n",
        "import extensions.api.streaming_api as streaming_api\n",
        "from modules import shared\n",
        "import asyncio\n",
        "import multiprocessing\n",
        "import os\n",
        "\n",
        "def runserv():\n",
        "   os.system('ssh -o ServerAliveInterval=60 -R 80:127.0.0.1:5000 serveo.net')\n",
        "\n",
        "def setup():\n",
        "   blocking_api.start_server(shared.args.api_blocking_port, share=shared.args.public_api)\n",
        "   streaming_api.start_server(shared.args.api_streaming_port, share=shared.args.public_api)\n",
        "\n",
        "   proc = multiprocessing.Process(target=runserv)\n",
        "   proc.start()\n",
        "\"\"\"\n",
        "\n",
        "cloudflare_api_script = \"\"\"\n",
        "import time\n",
        "\n",
        "import extensions.api.blocking_api as blocking_api\n",
        "import extensions.api.streaming_api as streaming_api\n",
        "from modules import shared\n",
        "\n",
        "def setup():\n",
        "    blocking_api.start_server(shared.args.api_blocking_port, share=shared.args.public_api, tunnel_id=shared.args.public_api_id)\n",
        "    time.sleep(5)\n",
        "    streaming_api.start_server(shared.args.api_streaming_port, share=shared.args.public_api, tunnel_id=shared.args.public_api_id)\n",
        "\"\"\"\n",
        "\n",
        "with open(script_file_path, \"w\") as script_file:\n",
        "    if use_cloudflare_public_api:\n",
        "      script_file.write(cloudflare_api_script)\n",
        "    else:\n",
        "      script_file.write(serveo_api_script)\n",
        "\n",
        "%cd {text_gen_dir}/text-generation-webui\n",
        "print(f\"\u001b[0;32mUpdated File : {script_file_path}\")\n",
        "\n",
        "if not use_cloudflare_public_api:\n",
        "  !mkdir -p ~/.ssh && echo \"Host *\" > ~/.ssh/config && echo \" StrictHostKeyChecking no\" >> ~/.ssh/config\n",
        "clear_output()\n",
        "print(f\"\u001b[0;32mUpdated File : {script_file_path}\")\n",
        "message('\\u2714 Downloaded','success', '50px')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Uk_ZzFD957nY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download Models || https://huggingface.co/TheBloke\n",
        "from IPython.display import clear_output\n",
        "\n",
        "model_path = \"TheBloke/Wizard-Vicuna-13B-Uncensored-SuperHOT-8K-GPTQ\" #@param {type:\"string\"}\n",
        "branch_path = \"main\" #@param {type:\"string\"}\n",
        "model_name = model_path.split('/')[-1]\n",
        "#@markdown ---\n",
        "#@markdown --> Ignore if downloading a GPTQ or AWQ model from TheBloke <--\n",
        "custom_install_path = \"\" #@param {type:\"string\"}\n",
        "custom_file_name = custom_install_path.split('/')[-1]\n",
        "#@markdown - Run cell multiple times if you have prerequisites for the model.\n",
        "#@markdown - Right click on model download link and copy link.\n",
        "#@markdown - Model Path is still required\n",
        "#@markdown ---\n",
        "#@markdown ---\n",
        "#@markdown Recommended Models (and their non-superHOT alternative) :\n",
        "#@markdown - TheBloke/Wizard-Vicuna-13B-Uncensored-SuperHOT-8K-GPTQ\n",
        "#@markdown - TheBloke/Nous-Hermes-13B-SuperHOT-8K-GPTQ\n",
        "#@markdown - TheBloke/Chronos-Hermes-13B-SuperHOT-8K-GPTQ\n",
        "#@markdown - TheBloke/Pygmalion-13B-SuperHOT-8K-GPTQ\n",
        "#@markdown - TheBloke/airoboros-l2-13b-gpt4-2.0-GGML [Not Tested]\n",
        "\n",
        "\n",
        "%cd {text_gen_dir}\n",
        "!apt-get -y install -qq aria2\n",
        "%cd {text_gen_dir}/text-generation-webui\n",
        "\n",
        "def message(msg, style, wdth):\n",
        "  message = widgets.Button(description=msg, disabled=True, button_style=style, layout=widgets.Layout(min_width=wdth))\n",
        "  display(message)\n",
        "\n",
        "def download():\n",
        "  if model_path == model_name or model_path.count(\"/\") > 1:\n",
        "    clear_output()\n",
        "    message('\\u2718 Incorrect Model Path','danger', \"250px\")\n",
        "    return\n",
        "\n",
        "  if branch_path == \"\" and custom_install_path == \"\":\n",
        "    clear_output()\n",
        "    message('\\u2718 Please enter a branch (ex. main)','danger', \"250px\")\n",
        "    return\n",
        "\n",
        "  files = [\"config.json\",\"generation_config.json\",\"special_tokens_map.json\",\"tokenizer.model\",\"tokenizer_config.json\",\"model.safetensors\"]\n",
        "\n",
        "  for FILE in files:\n",
        "    if FILE == \"tokenizer.model\" or FILE == \"model.safetensors\":\n",
        "      !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/{model_path}/resolve/{branch_path}/{FILE} -d {text_gen_dir}/text-generation-webui/models/{model_name} -o {FILE}\n",
        "    else:\n",
        "      !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/{model_path}/raw/{branch_path}/{FILE} -d {text_gen_dir}/text-generation-webui/models/{model_name} -o {FILE}\n",
        "\n",
        "  clear_output()\n",
        "  message('\\u2714 Downloaded','success', '50px')\n",
        "\n",
        "\n",
        "def custom_download():\n",
        "  if model_path == model_name or model_path.count(\"/\") > 1:\n",
        "    message('\\u2718 Wrong Path','danger', \"250px\")\n",
        "    return\n",
        "\n",
        "  !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M {custom_install_path} -d {text_gen_dir}/text-generation-webui/models/{model_name} -o {custom_file_name}\n",
        "  clear_output()\n",
        "  message('\\u2714 Downloaded','success', '50px')\n",
        "\n",
        "if custom_install_path == \"\":\n",
        "  download()\n",
        "else:\n",
        "  if custom_file_name != \"\":\n",
        "    custom_download()\n",
        "  else:\n",
        "    message('\\u2718 No File Name','danger', \"250px\")\n",
        "\n"
      ],
      "metadata": {
        "id": "oTSjwcwgKHqz",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Launch text-generation-webui || Reommended with SillyTavern\n",
        "import sys\n",
        "from IPython.display import clear_output\n",
        "\n",
        "model_load =  \"Wizard-Vicuna-13B-Uncensored-SuperHOT-8K-GPTQ\" #@param {type:\"string\"}\n",
        "public_link = False #@param {type:\"boolean\"}\n",
        "public_api = True #@param {type:\"boolean\"}\n",
        "max_seq_length = 8192 #@param {type:\"slider\", min:0, max:8192, step:512}\n",
        "compress_pos_embed = 4 #@param {type:\"slider\", min:0, max:4, step:1}\n",
        "#@markdown ---\n",
        "model_backend = \"ExLlama_hf\" #@param [\"ExLlama\", \"ExLlama_hf\", \"AutoGPTQ\", \"Llamacpp\", \"Llamacpp_hf\", \"rwkv\",\"ctransformers\"]\n",
        "gpu_layers = 43 #@param {type:\"slider\", min:0, max:43, step:1}\n",
        "offload_max_layers = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ---\n",
        "groupsize_128 = True #@param {type:\"boolean\"}\n",
        "load_4bit_models = True#@param {type:\"boolean\"}\n",
        "load_in_8bit = False #@param {type:\"boolean\"}\n",
        "use_xformers = False #@param {type:\"boolean\"}\n",
        "max_gpu_mem = 0 #@param {type:\"slider\", min:0, max:14000, step:500}\n",
        "#@markdown - Set to 0 to disable this\n",
        "\n",
        "%cd {text_gen_dir}/text-generation-webui\n",
        "\n",
        "\n",
        "if offload_max_layers:\n",
        "  gpu_layers = 1000000000\n",
        "\n",
        "if not public_link and not public_api:\n",
        "  print(\"\u001b[0;31mModel Will Not Load\")\n",
        "  sys.exit(1)\n",
        "\n",
        "\n",
        "params = set()\n",
        "\n",
        "if public_link:\n",
        "  params.add('--share')\n",
        "\n",
        "if public_api:\n",
        "  params.add('--public-api')\n",
        "\n",
        "params.add(f'--settings {text_gen_dir}/settings.yaml')\n",
        "params.add(f'--model {text_gen_dir}/text-generation-webui/models/{model_load}')\n",
        "params.add(f'--loader {model_backend}')\n",
        "params.add(f'--max_seq_len {max_seq_length}')\n",
        "params.add(f'--compress_pos_emb {compress_pos_embed}')\n",
        "\n",
        "\n",
        "if model_backend not in [\"ExLlama\", \"ExLlama_hf\", \"AutoGPTQ\"]:\n",
        "  params.add(f'--n-gpu-layers {gpu_layers}')\n",
        "\n",
        "if max_gpu_mem != 0:\n",
        "  params.add(f'----gpu-memory {max_gpu_mem}Mib')\n",
        "\n",
        "if load_4bit_models:\n",
        "  params.add('--wbits 4')\n",
        "\n",
        "if groupsize_128:\n",
        "  params.add('--groupsize 128')\n",
        "\n",
        "if use_xformers:\n",
        "  params.add('--xformers')\n",
        "\n",
        "clear_output()\n",
        "!python server.py {' '.join(params)}\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "t58uhazBmFRp",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}